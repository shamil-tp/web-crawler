# ğŸ•·ï¸ Web Crawler

A lightweight and efficient **Node.js Web Crawler** that crawls websites starting from a base URL, extracts internal links, and recursively processes pages.

This project demonstrates how search engines and indexing systems work at a fundamental level using JavaScript.

---

## ğŸš€ Features

- ğŸ” Crawl a website starting from a base URL  
- ğŸ”— Extract internal links from HTML pages  
- ğŸŒ Normalize URLs (avoid duplicates)  
- ğŸ“Š Track number of times each page is found  
- âš¡ Built with pure Node.js (Beginner Friendly)  
- ğŸ§  Easy to extend into a search engine or SEO analyzer  

---

## ğŸ› ï¸ Tech Stack

- **Node.js**
- **JavaScript (ES6+)**
- (Optional) Express.js if UI exists
- HTML Parser (if using libraries like jsdom or cheerio)

---

## ğŸ“ Project Structure

```
web-crawler/
â”‚
â”œâ”€â”€ index.js        # Entry point of the application
â”œâ”€â”€ crawl.js        # Crawling logic
â”œâ”€â”€ report.js       # Reporting logic (if exists)
â”œâ”€â”€ package.json    # Dependencies and scripts
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```



*(Adjust this structure if your actual repo differs.)*

---

## âš™ï¸ Installation

Make sure you have **Node.js v14+** installed.

```bash
# Clone the repository
git clone https://github.com/shamil-tp/web-crawler.git

# Navigate into project
cd web-crawler

# Install dependencies
npm install
